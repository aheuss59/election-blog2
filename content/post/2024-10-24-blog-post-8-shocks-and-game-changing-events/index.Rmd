---
title: 'Blog Post 8: Shocks and Game Changing Events'
author: Alex Heuss
date: '2024-10-24'
slug: blog-post-8-shocks-and-game-changing-events
categories: []
tags: []
---
```{r, load packages, echo=FALSE, message=FALSE}
library(car)
library(caret)
library(cowplot)
library(curl)
library(CVXR)
library(foreign)
library(geofacet)
library(glmnet)
library(haven)
library(janitor)
library(kableExtra)
library(maps)
library(mlr3)
library(randomForest)
library(ranger)
library(RColorBrewer)
library(rstan)
library(scales)
library(sf)
library(shinystan)
library(tidyverse)
library(viridis)
library(mice)
```

```{r, load in the nat data, echo=FALSE, message=FALSE}
# National Dataset
nat_pop <- read_csv("data/popvote_1948_2020.csv")
nat_party <- read_csv("data/national_party_id.csv") |>
  mutate(swing1 = percent - year_prior,
         swing1_2p = two_party_percent - year_prior_2p) |>
  arrange(year) |>
  group_by(party) |>
  mutate(prior_election = lag(percent, 1),
         prior_election_2p = lag(two_party_percent, 1),
         swing4 = percent - prior_election, 
         swing4_2p = two_party_percent - prior_election_2p)
nat_polls <- read_csv("data/national_polls_1968-2024.csv") |>
  mutate(party = ifelse(party == "DEM", "democrat", "republican")) |>
  filter(weeks_left <= 30) |>
  group_by(party, year, weeks_left) |>
  summarize(nat_poll = mean(poll_support)) |>
  pivot_wider(names_from = weeks_left, values_from = nat_poll)
colnames(nat_polls)[3:33] <- paste0("nat_weeks_left_", 0:30)
nat_econ_q2 <- read_csv("data/fred_econ.csv") |>
  filter(year >= 1968 & quarter == 2) |>
  rename(q2_gdp_growth = GDP_growth_quarterly,
         q2_rdpi_growth = RDPI_growth_quarterly) |>
  select(year, q2_gdp_growth, q2_rdpi_growth)
nat_econ_ann <- read_csv("data/fred_econ.csv") |>
  filter(year >= 1968) |>
  group_by(year) |>
  summarize(GDP = mean(GDP),
            RDPI = mean(RDPI),
            nat_unemployment = mean(unemployment),
            stock_adj_close = mean(sp500_adj_close))
nat_data <- 
  nat_pop |>
  left_join(nat_party, by = c("year", "party")) |>
  group_by(party) |>
  mutate(pv_lag1 = lag(pv, 1),
         pv_lag2 = lag(pv, 2)) |>
  left_join(nat_polls, by = c("year", "party")) |>
  filter(year >= 1968) |>
  left_join(nat_econ_q2, by = "year") |>
  left_join(nat_econ_ann, by = "year")

```

```{r, load in the state data, echo=FALSE, message=FALSE, warning=FALSE}
d_popvote <- read_csv("data/popvote_1948_2020.csv")
d_state_popvote <- read_csv("data/state_popvote_1948_2020.csv")

# Read elector distribution dataset. 
d_ec <- read_csv("data/corrected_ec_1948_2024.csv")

# Read ads datasets. 
campaign_spending <- read_csv("data/FEC_contributions_by_state_2008_2024.csv")

# Read polling data. 
d_polls <- read_csv("data/national_polls_1968-2024.csv")
d_state_polls <- read_csv("data/state_polls_1968-2024.csv")

# Read turnout data. 
d_turnout <- read_csv("data/state_turnout_1980_2022.csv")

d_campaign_spending <- d_state_popvote |> 
  mutate(state_abb = state.abb[match(d_state_popvote$state, state.name)]) |> 
  left_join(campaign_spending |> filter(party == "Democrat"), by = c("year" = "election_year", "state_abb" = "contribution_state")) |>
  filter(year >= 2008)

d_pollav_state <- d_state_polls |>
  group_by(year, state, party) |>
  mutate(mean_pollav = mean(poll_support, na.rm = TRUE)) |>
  top_n(1, poll_date) |>
  rename(latest_pollav = poll_support) |>
  select(-c(weeks_left, days_left, poll_date, candidate, before_convention)) |>
  pivot_wider(names_from = party, values_from = c(latest_pollav, mean_pollav))

d_state_econ <- 
  read_csv("data/Historical_State_Unemployment.csv") |>
  mutate(year = format(as.Date(date), "%Y")) |>
  mutate(year = as.double(year)) |>
  group_by(state, year) |>
  summarize(avg_state_unemployment = mean(unemployment)) |>
  left_join(nat_econ_ann, by = "year") |>
  left_join(nat_econ_q2, by = "year")

```

```{r, create state level historical party ID, echo=FALSE, message=FALSE, warning=FALSE}
stateabs <- d_ec |>
  filter(year == 2024) |>
  select(state, stateab)

stateabs_post1968 <- 
  d_ec |>
  filter(year >= 1968) |>
  select(year, state)

anes_party_id <- 
  read_csv("data/anes_timeseries_cdf_csv_20220916.csv") |>
  select(VCF0004, VCF0901b, VCF0302) |>
  rename(year = VCF0004,
         stateab = VCF0901b,
         party = VCF0302) |>
  filter(year >= 1968) |>
  left_join(stateabs, by = "stateab") |>
  select(year, state, party) |>
  mutate(party = case_when(party == 1 ~ "republican",
                           party == 2 ~ "independent",
                           party == 3 ~ "none",
                           party == 4 ~ "other",
                           party == 5 ~ "democrat")) |>
  filter(party == "republican" | party == "democrat" | party == "independent") |>
  group_by(year, state, party) |>
  summarize(count = n()) |>
  pivot_wider(names_from = party, values_from = count) |>
  mutate(democrat = ifelse(is.na(democrat), 0, democrat),
         republican = ifelse(is.na(republican), 0, republican),
         independent = ifelse(is.na(independent), 0, independent),
         state_total = sum(democrat, independent, republican),
         two_party_total = sum(democrat, republican),
         dem_perc = round((democrat/state_total)*100, 2),
         rep_perc = round((republican/state_total)*100, 2),
         ind_perc = round((independent/state_total)*100, 2),
         dem_2p_perc = round((democrat/two_party_total)*100, 2),
         rep_2p_perc = round((republican/two_party_total)*100, 2)) |>
  group_by(state)

d_state_party_id <- 
  stateabs_post1968 |>
  left_join(anes_party_id, by = c("year", "state")) |>
  filter(year != 2024) |>
  select(-democrat, -republican, -independent, -state_total, -two_party_total) |>
  mutate(total_voters = NA)
```

```{r, loop for 2024 Party ID, echo=FALSE, message=FALSE, warning=FALSE}
stateabbreviations <- unique((d_ec |> filter(stateab != "DC"))$stateab) 
i = 0
for (s in stateabbreviations) {
  # Load in the state's voter files
  voter_file_state_data <- 
    read_csv(paste0("data/state_1pc_samples_oct24/", s, "_sample.csv")) 
  
  # Count the number of voters in the voter files
  voter_file_total_voters <- nrow(voter_file_state_data)
  
  # Clean the data a lot
  voter_file_state_data <- 
    voter_file_state_data |>
    rename(stateab = sii_state) |>
    left_join(stateabs, by = "stateab") |>
    mutate(total_voters = voter_file_total_voters,
           dem_perc = ifelse(svi_party_modeled_rollup == "D", 1, 0),
           rep_perc = ifelse(svi_party_modeled_rollup == "R", 1, 0),
           ind_perc = ifelse(svi_party_modeled_rollup == "U", 1, 0))|>
    select(total_voters, dem_perc, rep_perc, ind_perc, state) |>
    group_by(state) |>
    summarize(total_voters = mean(total_voters),
              year = 2024,
              state = state,
              dem_perc = sum(dem_perc, na.rm = TRUE)/total_voters*100,
              rep_perc = sum(rep_perc, na.rm = TRUE)/total_voters*100,
              ind_perc = sum(ind_perc, na.rm = TRUE)/total_voters*100,
              dem_2p_perc = dem_perc / (dem_perc + rep_perc),
              rep_2p_perc = rep_perc / (rep_perc + dem_perc))
  
  voter_file_state_data <- slice(voter_file_state_data, 0:1)
  
  d_state_party_id <- rbind(d_state_party_id, voter_file_state_data)
}
```

```{r, clean state party data, echo=FALSE, message=FALSE}
d_state_party_id <- 
  d_state_party_id |>
  group_by(state) |>
  arrange(year) |>
  mutate(dem_perc_lag4 = lag(dem_perc, 1),
         rep_perc_lag4 = lag(rep_perc, 1),
         dem_perc_swing = dem_perc - dem_perc_lag4,
         rep_perc_swing = rep_perc - rep_perc_lag4)
  
```

## Discussion of Shocks and Game Changing Events

This week in class, we discussed shocks that sometimes are theorized to have played a role in election outcomes. These can be non-political shocks like natural disasters, or more political ones like a truly awful debate performance. 

In this election cycle, it seems like hardly anything can truly move the polls much. (My theory is that we are numb to crazy things happening now and it just doesn't phase us anymore). This election cycle, unpredictable shocks have almost been the norm. There have been two attempted assassination attempts on Former President Trump and President Biden dropped his re-election campaign extremely late in the game. Trump has said and done some very strange or alarming things. Two serious hurricanes hit the Southern coast in a matter of two weeks. Yet, almost none of these "game-changing" events have made a significant or lasting impact on the polls. 

The one shock that did make a clear difference was Biden's dropping out in late July. The Democrats went from polling barely above 40% to 45% in a matter of days. Looking at the graph below too, it does look like the first assassination attempt on Former President Trump also had an impact, but was masked somewhat by Harris' entrance into the race as the new Democratic nominee.

```{r, plot Biden dropout, echo=FALSE, message=FALSE}
read_csv("data/national_polls_1968-2024.csv") |>
  mutate(party = ifelse(party == "DEM", "democrat", "republican")) |>
  filter(weeks_left <= 30) |>
  filter(year == 2024) |>
  rename(Candidate = candidate) |>
  ggplot(aes(x = poll_date, y = poll_support, color=Candidate)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=as.Date("2024-07-21"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-07-13"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-06-15"), linetype="dashed") +
  labs(x = "", y = "Poll Support", title="2024 Presidential Polling: Biden Drops Out") +
  scale_color_manual(values = c("gray", "#00AEF3", "#E81B23")) + 
  annotate(x=as.Date("2024-06-15"), y=46, label="RNC", geom="label", size=3) +
  annotate(x=as.Date("2024-07-21"), y=47, label="Biden Drops", geom="label", size=3) +
  annotate(x=as.Date("2024-07-13"), y=48, label="1st Assasination Attempt", geom="label", size=3) +
  theme_bw() 

```

```{r, plot other shocks, echo=FALSE, message=FALSE}
read_csv("data/national_polls_1968-2024.csv") |>
  mutate(party = ifelse(party == "DEM", "democrat", "republican")) |>
  filter(weeks_left <= 30) |>
  filter(year == 2024 & poll_date > "2024-07-21") |>
  rename(Candidate = candidate) |>
  ggplot(aes(x = poll_date, y = poll_support, color=Candidate)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept=as.Date("2024-09-15"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-08-19"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-09-10"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-10-01"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-08-06"), linetype="dashed") +
  labs(x = "", y = "Poll Support", title="2024 Presidential Polling: Shocks Since Biden Dropped Out") +
  scale_color_manual(values = c("#00AEF3", "#E81B23")) + 
  annotate(x=as.Date("2024-09-15"),y=47,label="2nd Assasination Attempt",vjust=2,geom="label", size=3) +
  annotate(x=as.Date("2024-08-19"), y=46, label="DNC", geom="label", size=3) +
  annotate(x=as.Date("2024-09-10"), y=46, label="Presidential Debate", geom="label", size=3) +
  annotate(x=as.Date("2024-10-01"), y=47.5, label="VP Debate", geom="label", size=3) +
  annotate(x=as.Date("2024-08-06"), y=46.5, label="Walz VP Pick", geom="label", size=3) +
  theme_bw() 
```

One shock that may have been less predictable are the hurricanes that slammed Florida and North Carolina earlier this month. There are two different theories of voter behavior in situations like this. First, the idea that voters will punish the incumbent no matter their response, simply because it happened and it reduced their well-being. Theories like this are backed up by research like that of Achen and Bartels who found that voters punish the incumbent for shark attacks. On the other hand, another theory is that voters will reward incumbents who respond promptly and efficiently, which is supported by research like the Healy et al paper on tornados and economic impact, which found that voters punished incumbents when economic damage results from tornados, but only with strong effects when a state of emergency was not declared. 

```{r, load hurricane data, echo=FALSE, message=FALSE}
hurricanes <- read_csv("data/hurricanes_1996_2016.csv")
```

```{r, explore the hurricane data, echo=FALSE, message=FALSE, results=FALSE}
hurricanes |>
  filter(!(STATE %in% c("AMERICAN SAMOA", "GUAM", "PUERTO RICO", "VIRGIN ISLANDS"))) |>
  group_by(STATE, YEAR) |>
  summarize(count = n()) |>
  pivot_wider(names_from = YEAR, values_from = count) |>
  mutate(across(everything(), ~ ifelse(is.na(.), 0, .))) |>
  rename(State = STATE) |>
  mutate(State = str_to_title(State)) |>
  select(State, `1996`, `2000`, `2004`, `2008`, `2012`) |>
  knitr::kable(caption="Counties in Each State Impacted by a Hurricanes in Election Years")

```

```{r, look at polls in NC and Georgia to see if there are clear effects, fig.width=12, fig.height=5, echo=FALSE, message=FALSE}
d_state_polls |>
  filter(year == 2024 & state %in% c("North Carolina", "Georgia") & poll_date > "2024-07-21") |>
  rename(Candidate=candidate) |>
  ggplot(aes(x=poll_date, y=poll_support, color=Candidate)) +
  geom_point() + 
  geom_line() +
  geom_vline(xintercept=as.Date("2024-09-26"), linetype="dashed") +
  geom_vline(xintercept=as.Date("2024-10-09"), linetype="dashed") +
  scale_color_manual(values = c("#00AEF3", "#E81B23")) +
  facet_wrap(~state) +
  labs(x = "", y = "Polled Vote Share", title="Polling in Florida and North Carolina, Post-Biden Dropout") +
  annotate(x=as.Date("2024-09-26"), y=46.5, label="Helene hits in FL", geom="label", size=3) +
  annotate(x=as.Date("2024-10-09"), y=45, label="Milton hits in FL", geom="label", size=3) +
  theme_bw()

```

If we had to draw conclusions from a these two graphs, we would probably conclude that the hurricanes reduced support for the Democrats in these two states, specifically Helene. That said, it doesn't make a whole lot of sense to draw those conclusions from this evidence alone given confounding variables. Communities that were hit hard by the hurricanes or their aftermath probably don't have a lot of people in them right now who are just sitting at home answering poll questions. They are out there trying to help build back their communities. It is unlikely we will be able to truly look at the impact of these events until after election day. 

```{r, cleaning the hurricane data, echo=FALSE, message=FALSE, warning=FALSE}
clean_hurricanes <- 
  hurricanes |>
  mutate(state = str_to_title(STATE),
         month_numb = case_when(MONTH_NAME == "January" ~ "01",
                                MONTH_NAME == "February" ~ "02",
                                MONTH_NAME == "March" ~ "03",
                                MONTH_NAME == "April" ~ "04",
                                MONTH_NAME == "May" ~ "05",
                                MONTH_NAME == "June" ~ "06",
                                MONTH_NAME == "July" ~ "07",
                                MONTH_NAME == "August" ~ "08",
                                MONTH_NAME == "September" ~ "09",
                                MONTH_NAME == "October" ~ "10",
                                MONTH_NAME == "November" ~ "11",
                                MONTH_NAME == "December" ~ "12"),
         begin_day = sprintf("%02d", BEGIN_DAY),
         ymd = paste0(month_numb, begin_day, YEAR),
         damage_property = case_when(grepl("M$", DAMAGE_PROPERTY) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_PROPERTY)) * 1e6,
                                     grepl("B$", DAMAGE_PROPERTY) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_PROPERTY)) * 1e9,
                                     grepl("K$", DAMAGE_PROPERTY) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_PROPERTY)) * 1e3,
                                     TRUE ~ as.numeric(DAMAGE_PROPERTY)),
         damage_crops = case_when(grepl("M$", DAMAGE_CROPS) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_CROPS)) * 1e6,
                                  grepl("B$", DAMAGE_CROPS) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_CROPS)) * 1e9,
                                  grepl("K$", DAMAGE_CROPS) ~ as.numeric(gsub("[^0-9.]", "", DAMAGE_CROPS)) * 1e3,
                                  TRUE ~ as.numeric(DAMAGE_CROPS))) |>
  select(-EVENT_TYPE, -BEGIN_TIME, -END_YEARMONTH, -END_DAY, -END_TIME, -CZ_TYPE, -STATE) |>
  group_by(state, ymd) |>
  summarize(year = mean(YEAR),
            month = mean(as.numeric(month_numb)),
            day = mean(as.numeric(begin_day)), 
            counties_affected = n(),
            injuries_direct = sum(INJURIES_DIRECT),
            injuries_indirect = sum(INJURIES_INDIRECT),
            deaths_direct = sum(DEATHS_DIRECT),
            deaths_indirect = sum(DEATHS_INDIRECT),
            magnitude = mean(MAGNITUDE, na.rm=TRUE),
            damage_property = sum(damage_property),
            damage_crops = sum(damage_crops)) |>
  filter(!(state %in% c("American Samoa", "Guam", "Puerto Rico", "Virgin Islands")))
  
```

```{r, cleaning the hurricane data differently, echo=FALSE, message=FALSE}
cleaner_hurricanes <- 
  clean_hurricanes |>
  mutate(july_hurricane=ifelse(month == 7, 1, 0),
         aug_hurricane = ifelse(month == 8, 1, 0),
         sep_hurricane = ifelse(month == 9, 1, 0),
         oct_hurricane = ifelse(month == 10, 1, 0)) |>
  select(state, year, counties_affected, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, damage_property, damage_crops, july_hurricane, aug_hurricane, sep_hurricane, oct_hurricane) |>
  group_by(state, year) |>
  summarize(counties_affected = sum(counties_affected),
            injuries_direct = sum(injuries_direct),
            injuries_indirect = sum(injuries_indirect),
            deaths_direct = sum(deaths_direct),
            deaths_indirect = sum(deaths_indirect),
            damage_property = sum(damage_property, na.rm=TRUE),
            damage_crops = sum(damage_crops, na.rm=TRUE),
            july_hurricane = sum(july_hurricane),
            aug_hurricane = sum(aug_hurricane),
            sep_hurricane = sum(sep_hurricane),
            oct_hurricane = sum(oct_hurricane))
  
```

Looking at historical data for the effects of hurricanes on state-level popular vote, effects are unclear. Below is the coefficient breakdown for a simple linear model predicting state-level popular vote share for Democratic candidates based on their interaction terms between incumbency status or incumbent party status and whether a hurricane hit the state or how many hurricanes hit the state between July and October of that year (4 separate interaction terms). 

```{r, running a simple linear regression on hurricanes, echo=FALSE, message=FALSE}
nat_pop_hurr <- 
  nat_pop |>
  filter(party == "democrat") |>
  select(incumbent, incumbent_party, year)

pop_hurricanes <- 
  d_state_popvote |>
  left_join(cleaner_hurricanes, by = c("state", "year")) |>
  left_join(nat_pop_hurr, by = "year") |>
  filter(year >= 1996 & state %in% unique(cleaner_hurricanes$state)) |>
  mutate(total_hurricanes = july_hurricane + aug_hurricane + sep_hurricane + oct_hurricane,
         any_hurricanes = ifelse(total_hurricanes > 0, TRUE, FALSE),
         incumbent_any_interaction = ifelse(is.na(any_hurricanes), 0, incumbent*any_hurricanes),
         incumbent_total_interaction = ifelse(is.na(total_hurricanes), 0, incumbent*total_hurricanes),
         party_any_interaction = ifelse(is.na(any_hurricanes), 0, incumbent_party*any_hurricanes),
         party_total_interaction = ifelse(is.na(total_hurricanes), 0, incumbent_party*total_hurricanes))
  

hurricane_lm <- lm(D_pv ~ D_pv_lag1 + D_pv_lag2 + incumbent_total_interaction + incumbent_any_interaction + party_total_interaction + party_any_interaction + state, data = pop_hurricanes)

summary(hurricane_lm)

```

We can see that none of the hurricane relevant variables were statistically significant predictors for incumbent vote share. This finding held true when using all states or just the states in which hurricanes typically impact. It also held true for both predictions of Democratic and Republican popular vote share. 

For a visual of the relationship, we see a very slight negative relationship between interaction of number of hurricanes and incumbency and Democratic popular vote share, but with a very large standard error. 

```{r, a graph about hurricanes, echo=FALSE, message=FALSE}
pop_hurricanes |>
  ggplot(aes(x = party_total_interaction, y = D_pv)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Visualizing Incumbency and Amount of Hurricane Interaction",
       x = "Incumbency x Number of Hurricanes",
       y = "State Popular Vote")
```

The next graph gives the same visualization but for the interaction between incumbency and the binary variable of whether a state was hit by a hurricane. It too shows virtually no relationship. 

```{r, another graph for hurricanes, echo=FALSE, message=FALSE}
pop_hurricanes |>
  ggplot(aes(x = party_any_interaction, y = D_pv)) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Visualizing Incumbency and Hurricane Interaction",
       x = "Incumbency x Occurance of Hurricanes",
       y = "State Popular Vote")
```

There are a few reasons this might be the case. I would expect that the most prominent is a lack of training data. The data for hurricanes only goes back to 1996, encompassing only seven elections and only a few instances of hurricanes in each. It also may be the case that my analysis just wasn't granular enough and really needed to be at the county-level. 

## National Popular Vote Prediction

I did not change anything in my model for predicting the national popular vote this week, but the national polling averages were updated.

```{r, perform national pop vote lasso prediction, echo=FALSE, message=FALSE, warning=FALSE}
# Subset to the weeks_left we have polling data for
nat_data_subset <- 
  nat_data |>
  select(-c(paste0("nat_weeks_left_", 0:2)),
         -candidate, -winner)
# Train and Test splits
nat_train <- 
  nat_data_subset |> 
  filter(year <= 2020)
nat_test <- 
  nat_data_subset |>
  filter(year == 2024)
x.train <- nat_train |>
  ungroup() |> 
  select(-pv, -pv2p, -year) |>
  as.matrix()
y.train <- nat_train$pv
x.test <- nat_test |>
  ungroup() |> 
  select(-pv, -pv2p, -year) |>
  as.matrix()

# Use Lasso for National Popular Vote
lasso.nat <- glmnet(x = x.train, y = y.train, alpha = 1)
set.seed(02138)
cv.lasso.nat <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
lambda.min.lasso <- cv.lasso.nat$lambda.min
mse.lasso <- mean((predict(lasso.nat, s = lambda.min.lasso, newx = x.train) - y.train)^2)

lasso.coef.model <- glmnet(x = x.train, y = y.train, alpha = 1, lambda = lambda.min.lasso)
coef(lasso.coef.model)

```

This week, I wanted to take a closer look at which variables are chosen as significant by my LASSO model. Percentage of independent voters, party identification swing from the previous year, popular vote share from the last election, and polls at weeks 3, 5, and 18 were chosen as the most important predictors.

```{r, predict, echo=FALSE, message=FALSE, warning=FALSE}
(lasso.nat.pred <- predict(lasso.nat, s = lambda.min.lasso, newx = x.test))
```

This week's national vote prediction predicts an even tighter popular vote margin for Kamala Harris, who would win 49.17% of the popular vote compared to Trump's 48.04%. Trump gained on Harris from last week's prediction by 0.03 percentage points. 

## Electoral College Prediction

This week, I again did not make major changes to my electoral college prediction model. 538 has not updated it's polling averages from the ones I used last week, so those were held constant in this week's model as well. We were given access to estimated voter roll party identification, so I was able to update my data to use a more accurate value for party identification swing. 

```{r, predict electoral college for states with polls, echo=FALSE, message=FALSE}
# Merge data.
d <- d_pollav_state |>
  left_join(d_state_party_id, by = c("year", "state")) |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  left_join(d_popvote |> filter(party == "democrat"), by = "year") |>
  left_join(d_turnout, by = c("year", "state")) |>
  left_join(d_state_econ, by = c("state", "year")) |>
  filter(year >= 1980) |>
  ungroup()

# Take a minute to figure out an imputation method for states with open primaries whose 2024 partisan identification is flawed

# Sequester states for which we have polling data for 2024.

states.2024 <- unique(d$state[d$year == 2024])
states.2024 <- states.2024[-which(states.2024 == "Nebraska Cd 2")]

d<- 
  d |>
  filter(state %in% states.2024)

# Separate into training and testing for simple poll prediction model. 
d.train <- d |> filter(year < 2024) |> select(year, state, D_pv, latest_pollav_DEM, mean_pollav_DEM, 
                                              D_pv_lag1, D_pv_lag2, incumbent_party, avg_state_unemployment, q2_gdp_growth, dem_perc, rep_perc, ind_perc, dem_perc_lag4, rep_perc_lag4, dem_perc_swing, rep_perc_swing) |> drop_na()

d.test <- d |> filter(year == 2024) |> select(year, state, D_pv, latest_pollav_DEM, mean_pollav_DEM, 
                                              D_pv_lag1, D_pv_lag2, incumbent_party, avg_state_unemployment, q2_gdp_growth, dem_perc, rep_perc, ind_perc, dem_perc_lag4, rep_perc_lag4, dem_perc_swing, rep_perc_swing)

# Add back in lagged vote share for 2024. 
t <- d |> 
  filter(year >= 2016) |> 
  arrange(year) |> 
  group_by(state) |> 
  mutate(
    D_pv_lag1 = lag(D_pv, 1),
    R_pv_lag1 = lag(R_pv, 1), 
    D_pv_lag2 = lag(D_pv, 2),
    R_pv_lag2 = lag(R_pv, 2)) |> 
  filter(year == 2024) |> 
  select(state, year, D_pv, R_pv, D_pv_lag1, R_pv_lag1, D_pv_lag2, R_pv_lag2)

# Subset testing data to only relevant variables for our simple model. 
d.test <- d.test |> 
  select(-c(D_pv, D_pv_lag1, D_pv_lag2)) |> 
  left_join(t, by = c("state", "year"))

# Standard frequentist linear regression. 
reg.ols <- lm(D_pv ~ latest_pollav_DEM + mean_pollav_DEM + D_pv_lag1 + D_pv_lag2 + incumbent_party + q2_gdp_growth + avg_state_unemployment + dem_perc_swing + rep_perc_swing + state, 
              data = d.train)
pred.ols.dem <- predict(reg.ols, newdata = d.test)

# Create dataset to summarize winners and EC vote distributions. 
win_pred <- data.frame(state = d.test$state,
                       year = rep(2024, length(d.test$state)),
                       simp_pred_dem = pred.ols.dem,
                       simp_pred_rep = 100 - pred.ols.dem) |> 
            mutate(winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")) |>
            left_join(d_ec, by = c("state", "year"))

```

```{r, predict electoral college for states without polls, echo=FALSE, message=FALSE}
missing_state_polls <- 
  read_csv("data/missing_poll_states_csv.csv")
missing_state_polls <- 
  missing_state_polls |>
  filter(party == "DEM" & !state %in% states.2024) |>
  mutate(latest_pollav_REP = poll_support,
         latest_pollav_DEM = poll_support,
         mean_pollav_REP = poll_support,
         mean_pollav_DEM = poll_support) |>
  select(year, state, latest_pollav_REP, latest_pollav_DEM, mean_pollav_REP, mean_pollav_DEM)
missing_state_polls <- rbind(d_pollav_state, missing_state_polls)

d_no_polls <- missing_state_polls |>
  left_join(d_state_party_id, by = c("year", "state")) |>
  left_join(d_state_popvote, by = c("year", "state")) |>
  left_join(d_popvote |> filter(party == "democrat"), by = "year") |>
  left_join(d_turnout, by = c("year", "state")) |>
  left_join(d_state_econ, by = c("state", "year")) |>
  filter(year >= 1980 & state != "Nebraska Cd 2") |>
  ungroup()|>
  filter(!state %in% states.2024)

# Separate into training and testing for simple poll prediction model. 
d.train.nopolls <- d_no_polls |> filter(year < 2024) |> select(year, state, D_pv, 
                                              D_pv_lag1, D_pv_lag2, incumbent_party, avg_state_unemployment, q2_gdp_growth, dem_perc, rep_perc, ind_perc, dem_perc_lag4, rep_perc_lag4, dem_perc_swing, rep_perc_swing) |> drop_na()
d.test.nopolls <- d_no_polls |> filter(year == 2024) |> select(year, state, D_pv, 
                                              D_pv_lag1, D_pv_lag2, incumbent_party, avg_state_unemployment, q2_gdp_growth, dem_perc, rep_perc, ind_perc, dem_perc_lag4, rep_perc_lag4, dem_perc_swing, rep_perc_swing)

# Add back in lagged vote share for 2024. 
t.nopolls <- d_no_polls |> 
  filter(year >= 2016) |> 
  arrange(year) |> 
  group_by(state) |> 
  mutate(
    D_pv_lag1 = lag(D_pv, 1),
    R_pv_lag1 = lag(R_pv, 1), 
    D_pv_lag2 = lag(D_pv, 2),
    R_pv_lag2 = lag(R_pv, 2)) |> 
  filter(year == 2024) |> 
  select(state, year, D_pv, R_pv, D_pv_lag1, R_pv_lag1, D_pv_lag2, R_pv_lag2)

# Subset testing data to only relevant variables for our simple model. 
d.test.nopolls <- d.test.nopolls |> 
  select(-c(D_pv, D_pv_lag1, D_pv_lag2)) |> 
  left_join(t.nopolls, by = c("state", "year"))

# Standard frequentist linear regression. 
reg.ols.nopolls <- lm(D_pv ~ D_pv_lag1 + D_pv_lag2 + incumbent_party + q2_gdp_growth + avg_state_unemployment + dem_perc_swing + rep_perc_swing + state, 
              data = d.train.nopolls)
pred.ols.dem.nopolls <- predict(reg.ols.nopolls, newdata = d.test.nopolls)

# Create dataset to summarize winners and EC vote distributions. 
win_pred_nopolls <- data.frame(state = d.test.nopolls$state,
                       year = rep(2024, length(d.test.nopolls$state)),
                       simp_pred_dem = pred.ols.dem.nopolls,
                       simp_pred_rep = 100 - pred.ols.dem.nopolls) |> 
            mutate(winner = ifelse(simp_pred_dem > simp_pred_rep, "Democrat", "Republican")) |>
            left_join(d_ec, by = c("state", "year"))

```

```{r, show coefficients, echo=FALSE, message=FALSE}
summary(reg.ols)
summary(reg.ols.nopolls)
```

Looking at the models for states with and without polls, we see that the most statistically significant predictors are latest polls (if applicable), a state's recent vote share, incumbency, and some state fixed effects. The adjusted R squared for my model with polls is very high at 0.9155, which suggests that it fits the observed data very well, but definitely might be overfitting. On the other hand, my model without polls has an adjusted R squared of 0.724, which is lower and might encourage me to look for another variable that could explain more of the variance. 

```{r, breakdown electoral college results, echo=FALSE, message=FALSE}
all_preds <- rbind(win_pred, win_pred_nopolls)

states_map <- map_data("state")
all_preds |>
  rename(Winner = winner) |>
  group_by(Winner) |>
  summarize(`States Won` = n(), Electors = sum(electors)) |>
  knitr::kable()
```

My electoral college prediction remains the same this week, with Trump winning the electoral college 272-266. (Reminder that DC is not included in my model, but will almost certainly go blue)

```{r, swing state specific breakdown, echo=FALSE, message=FALSE}
all_preds |>
  filter(state %in% c("Michigan", "Wisconsin", "Pennsylvania", "Georgia", "North Carolina", "Arizona", "Nevada")) |>
  mutate(Margin = simp_pred_dem - simp_pred_rep) |>
  rename(State = state, Winner = winner) |>
  select(State, Winner, Margin) |>
  arrange(Winner) |>
  knitr::kable()
```

Looking at the specific state-level predictions for swing states this week, most of the states are further red than last week with the correction of my partisan swing variable. 

A full look at the electoral college mapping of predictions is below. 

```{r, map, echo=FALSE, message=FALSE}
all_preds |>
  mutate(region = tolower(state),
         Margin = simp_pred_dem - simp_pred_rep) |>
  left_join(states_map, by = "region") |>
  ggplot(aes(long, lat, group=group)) + 
  geom_polygon(aes(fill = Margin), color = "black") + 
  scale_fill_gradient2(midpoint = 0, low = "#E81B23", mid = "white", high = "#00AEF3") +
  labs(title = "Electoral College Prediction Map") +
  theme_void()
```











